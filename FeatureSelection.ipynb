{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nngu0123/ADS2002/blob/main/FeatureSelection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnEDtT58ZlZF"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78LMFti3ZlZG"
      },
      "source": [
        "For large datasets, feature selection, where we try to choose the best features to fit our models to, is an important preprocessing step. The benefits of this are reducing data storage requirements, reducing calculation times for fitting, and improving the robustness of the model. We have previously investigated embedded methods such as Lasso or Ridge regularization for linear and logistic regression and SVM models which implicitly do this. Here we will investigate some basic techniques using the scikit-learn library which enable us to undertake feature selection. These methods fall into the categories of filter and wrapper methods. For filter methods the filtering is done as a preprocessing step and is independent of the final model we create. Whereas for wrapper methods we require a machine learning algorithm and use the metrics of that model to guide the feature selection.\n",
        "\n",
        "To investigate these basic feature selection methods we will use the Wisconsin Breast Cancer dataset which you will need to download from [Monash Gitlab](https://gitlab.erc.monash.edu.au/bads/data-challenges-resources/-/blob/main/Machine-Learning/Supervised-Methods/kaggle-wisconsin-cancer.csv) or [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data). This datasets records 30 physical features of digitized images of cells, and classifies them as benign (B) or malignant (M).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9wVG6JxZlZG"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-RYvSqVZlZG"
      },
      "source": [
        "* Inputs\n",
        "* Multicollinearity\n",
        "* Variance threshold\n",
        "* SelectKBest\n",
        "* Select from model\n",
        "* Recursive feature elimination\n",
        "* Sequential feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_smqGT1ZlZG"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzn-8kruZlZG"
      },
      "source": [
        "We first import the standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x2IEB0_gZlZG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjooPD3XZlZH"
      },
      "source": [
        "Read in the csv file to a pandas dataframe and show the descriptive statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "vvm_HQojZlZH",
        "outputId": "016008df-f8b6-48b9-f86c-bddd7b8a9b3b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5b4fc91db026>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kaggle-wisconsin-cancer.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read the dataset in to a pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle-wisconsin-cancer.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('kaggle-wisconsin-cancer.csv') # read the dataset in to a pandas dataframe\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pOcT8ovZlZH"
      },
      "source": [
        "The diagnosis corresponds to M (malignant) or B (benign), with 37% M. For this problem we want to minimize false negatives, i.e., predictions that the tumour is benign, when it is actually malignant. Hence we want to maximize the recall = TP/(TP+FN).\n",
        "\n",
        "We create a dataframe of the features, and undertake some minimal cleaning. Then we convert the diagnosis from categories to a binary classification where 1 represents M and 0 represents B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqV_K2IxZlZH"
      },
      "outputs": [],
      "source": [
        "features = df.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)\n",
        "labels = df[\"diagnosis\"].astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHN9fi2OZlZH"
      },
      "source": [
        "## Multicollinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfIg9hEbZlZH"
      },
      "source": [
        "The first filtering method we should undertake is to remove features which have very high absolute correlation to reduce the problem of collinearity. We can determine whether this is required by displaying the maximum and minimum values of the correlation matrix for the features. This demonstrates that some features have high positive correlation, but high negative correlation is not a problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuj5lFkpZlZH"
      },
      "outputs": [],
      "source": [
        "corrs = features.corr() # calculate the correlation table\n",
        "upper_tri = corrs.where(np.triu(np.ones(corrs.shape),k=1).astype(np.bool))\n",
        "print('Minimum correlation is',np.round(upper_tri.min().min(),3))\n",
        "print('Maximum correlation is',np.round(upper_tri.max().max(),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4civRebOZlZI"
      },
      "source": [
        "To get an idea of the correlated features we can display the correlation matrix. The features that are highly correlated in this case are the biege entries, which corresponds to approximately 7 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibNRQwpNZlZI"
      },
      "outputs": [],
      "source": [
        "# as this is a symmetric table, set up a mask so that we only plot values below the main diagonal\n",
        "mask = np.triu(np.ones_like(corrs, dtype=np.bool))\n",
        "f, ax = plt.subplots(figsize=(10, 8)) # initialise the plots and axes\n",
        "# plot the correlations as a seaborn heatmap, with a colourbar\n",
        "sns.heatmap(corrs, mask=mask, center=0, annot=False, square=True, linewidths=.5)\n",
        "# do some fiddling so that the top and bottom are not obscured\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim(bottom + 0.5, top - 0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWUokN3qZlZI"
      },
      "source": [
        "We can then iterate through the columns of the matrix and drop any columns which have an absolute correlation greater than some critical value. Those columns can be displayed and then dropped from the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPLlOErcZlZI"
      },
      "outputs": [],
      "source": [
        "upper_tri = corrs.where(np.triu(np.ones(corrs.shape),k=1).astype(np.bool))\n",
        "to_drop = [column for column in upper_tri.columns if any(np.abs(upper_tri[column]) > 0.95)]\n",
        "print(to_drop)\n",
        "tfeatures = features.drop(features[to_drop], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16gmRJZiZlZI"
      },
      "source": [
        "As a check, we should compare the model accuracy with the full feature set and the reduced feature set. To do this we introduce a function which fits an sklearn model and calculates the accuracy, precision and recall. This uses the sklearn `StandardScalar` as a pipeline to first normalize the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWCJ3rcTZlZI"
      },
      "outputs": [],
      "source": [
        "def model_accuracy(model, features, labels, randomnumb=42):\n",
        "    '''Convenience function to fit an sklearn model and calculate the accuracy, precision and recall'''\n",
        "    import numpy as np\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score # import the score functions\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=randomnumb)\n",
        "    pipe = make_pipeline(StandardScaler(), model)\n",
        "    pipe.fit(X_train, y_train)  # apply scaling on training data\n",
        "    y_pred= pipe.predict(X_test) # calculate the predicted values of the model for the test features\n",
        "\n",
        "    accuracy = np.round(accuracy_score(y_test, y_pred),3)\n",
        "    precision = np.round(precision_score(y_test, y_pred),3)\n",
        "    recall = np.round(recall_score(y_test, y_pred),3)\n",
        "\n",
        "    print('Metrics for', model)\n",
        "    print('Accuracy ', accuracy)\n",
        "    print('Precision ', precision)\n",
        "    print('Recall ', recall)\n",
        "    print('\\n')\n",
        "\n",
        "    return (accuracy, precision, recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ydRNumzZlZI"
      },
      "source": [
        "We can then use a simple logistic regression model to compare the results for the full dataset and the trimmed dataset. This doesn't take into account cross-validation or optimization of parameters. For each of the accuracy measurements the reduction is approximately 1.5%, so in what follows we will use the trimmed dataset (tfeatures)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzriNan1ZlZI"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print('Full set of features')\n",
        "logreg = LogisticRegression(penalty='none')\n",
        "model_accuracy(logreg, features, labels)\n",
        "\n",
        "print('Trimmed set of features')\n",
        "logreg = LogisticRegression(penalty='none')\n",
        "model_accuracy(logreg, tfeatures, labels);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci-RpFq2ZlZJ"
      },
      "source": [
        "## Variance Threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCLYr5mCZlZJ"
      },
      "source": [
        "The simplest filtering of features that we can perform is to remove the features with the lowest variance or standard deviation. The rationale is that at least for linear models the magnitude of the model coefficients will be proportional to the variance of that feature. We can see that a number of features have very low variance and are candidates for being eliminated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUcOMi6_ZlZJ"
      },
      "outputs": [],
      "source": [
        "std_scores = pd.DataFrame(tfeatures.std())\n",
        "std_scores.columns = ['Std']  # name output columns\n",
        "std_scores.sort_values(by='Std', inplace=True)\n",
        "\n",
        "std_scores.plot(kind='barh', figsize=(8, 7)) # plot these as a bar plot\n",
        "plt.axvline(x=0.1, color='.5') # add the reference line y = 0\n",
        "plt.xlabel('Standard Deviation');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvarMMZlZlZJ"
      },
      "source": [
        "If we include only features with a variance greater that 0.1, this reduces our dataset to 7 features. The recall reduces by another 2%, though the precision and accuracy both improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P_MfVATZlZJ"
      },
      "outputs": [],
      "source": [
        "features_highvar = tfeatures.loc[:, tfeatures.std() > 0.1]\n",
        "\n",
        "print(features_highvar.shape)\n",
        "print('\\n')\n",
        "print('Variance threshold')\n",
        "logreg = LogisticRegression(penalty='none')\n",
        "accuracy, precision, recall = model_accuracy(logreg, features_highvar, labels);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hADOPhkMZlZJ"
      },
      "source": [
        "## SelectKBest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiDY7BCiZlZJ"
      },
      "source": [
        "sklearn provides a number of functions which allow feature selection based on statistical tests comparing the feature set and the data labels. We will just consider `SelectKBest` here, which chooses the features with the highest statistical scores. To investigate other methods, see the sklearn documentation. The two common tests for classification problems are the chi2 (chi squared) test and the f_classif (ANOVA F-value) test. The f_classif is the default test for classification problems. For regression problems you would use f_regression, which calculates the correlation between features and labels.\n",
        "\n",
        "`SelectKBest` works the same as other sklearn models where you instantiate the model, then fit the data to the model and then extract the information. In this case we just want the scores, so we don't specify how many features to extract. We can then extract the scores and display then based on their chi2 ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "jvhYcycnZlZJ",
        "outputId": "476a3c2d-c3db-485a-8d3c-26004f08f8ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-166d67743d7b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeatures_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfeatures' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2, f_regression\n",
        "\n",
        "features_new = SelectKBest(chi2).fit(tfeatures, labels)\n",
        "df_scores = pd.DataFrame(features_new.scores_)\n",
        "df_columns = pd.DataFrame(tfeatures.columns)\n",
        "# concatenate dataframes\n",
        "chi2_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "chi2_scores.columns = ['Feature_Name','Chi2 Score']  # name output columns\n",
        "chi2_scores.set_index('Feature_Name', inplace=True)\n",
        "\n",
        "chi2_scores.sort_values(by='Chi2 Score').plot(kind='barh', figsize=(8, 7)) # plot these as a bar plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtFABGTZlZJ"
      },
      "source": [
        "To extract the k best features we then specify the number of features and fit and transform the data. We can then fit a logistic regression model to this transformed data. As can seen for this dataset the metrics all are very high, which suggests that the model is accurate and robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4Av1XutZlZJ"
      },
      "outputs": [],
      "source": [
        "features_chi2 = SelectKBest(chi2, k=4).fit_transform(tfeatures, labels)\n",
        "\n",
        "print('Chi2 test for SelectKBest with 4 features')\n",
        "logreg = LogisticRegression(penalty='none')\n",
        "accuracy, precision, recall = model_accuracy(logreg, features_chi2, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQbFP295ZlZK"
      },
      "source": [
        "We can repeat the process using the ANOVA F-value test. In this case the distribution of scores is not quite as strong as for the chi2 test, and the accuracy metrics all decrease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EdPDzL5ZlZK"
      },
      "outputs": [],
      "source": [
        "features_new = SelectKBest().fit(tfeatures, labels)\n",
        "df_scores = pd.DataFrame(features_new.scores_)\n",
        "df_columns = pd.DataFrame(tfeatures.columns)\n",
        "# concatenate dataframes\n",
        "anova_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "anova_scores.columns = ['Feature_Name','ANOVA Score']  # name output columns\n",
        "\n",
        "anova_scores.set_index('Feature_Name', inplace=True)\n",
        "anova_scores.sort_values(by='ANOVA Score').plot(kind='barh', figsize=(8, 7)) # plot these as a bar plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVxm2qSwZlZK"
      },
      "outputs": [],
      "source": [
        "features_anova = SelectKBest(k=4).fit_transform(tfeatures, labels)\n",
        "\n",
        "print('ANOVA F test for SelectKBest with 4 features')\n",
        "logreg = LogisticRegression(penalty='none')\n",
        "accuracy, precision, recall = model_accuracy(logreg, features_anova, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd044iW1ZlZK"
      },
      "source": [
        "As a comparison we can compare the scores for the three tests we have so far undertaken. Generally as one increases the other also increase, but there are some anomalies for the three different tests.\n",
        "\n",
        "In this case, since each test is simple and quick, the best approach is to try each method and use the method that gives the best balance between bias and variance for your particular dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8juMnQZvZlZK"
      },
      "outputs": [],
      "source": [
        "scores = pd.concat([std_scores, chi2_scores, anova_scores], axis=1)\n",
        "scores.columns = ['Std', 'Chi2', 'Anova']\n",
        "scores.sort_values(by='Std')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmVkw72ZlZK"
      },
      "source": [
        "## SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuTlYUnlZlZK"
      },
      "source": [
        "The sklearn function `SelectFromModel` is our first example of a wrapper routine. This takes the output of a model and uses attributes of the model to rank the features and then extract features based on some threshold. The simplest way to understand this for Decision Tree type models which output a feature importance for each feature. This can then be used to rank the features, such that those with the highest importance are retained. The default threshold is to keep features with feature importance greater than the mean value.\n",
        "\n",
        "`SelectFromModel` can also use the coefficients of a model such a linear or logistic regression or SVM to rank the features.\n",
        "\n",
        "We will use `RandomForestClassifier` to illustrate the use of `SelectFromModel`, hence we first need to create a baseline for the accuracy metrics. Again, this baseline does not use optimization of parameters or cross-validation, but is rather just for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__z_wPWjZlZU"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "accuracy, precision, recall = model_accuracy(rfc, tfeatures, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcHcdYqaZlZU"
      },
      "source": [
        "To use this model we need to specify an estimator, which is in this case `RandomForestClassifier`, and a threshold for choosing features. For this classifier the feature importance will be used to select features, whereas for `LogisticRegression` the model coefficients would be used. The default for the threshold is the mean of the feature importance. This could be the median or a fraction of the mean, by specifying, for example, `threshold=\"0.5*mean\"`. We then fit the model to the features and labels. For comparison with later routines we display the time taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RDVpmf0ZlZU"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "selector = SelectFromModel(estimator=RandomForestClassifier(), threshold=\"mean\")\n",
        "%timeit selector.fit(tfeatures, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB0tJAieZlZU"
      },
      "source": [
        "The scores for each feature then can be extracted using the object `estimator_.feature_importances_`, and the critical value for the scores can be extracted using the object `threshold_`. Then we can view the ranking of the features. Using the mean value gives that six features are selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJlGjEciZlZU"
      },
      "outputs": [],
      "source": [
        "df_scores = pd.DataFrame(selector.estimator_.feature_importances_)\n",
        "msfm = selector.threshold_\n",
        "df_columns = pd.DataFrame(tfeatures.columns)\n",
        "# concatenate dataframes\n",
        "sfm_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "sfm_scores.columns = ['Feature_Name','Feature Importance']  # name output columns\n",
        "sfm_scores.set_index('Feature_Name', inplace=True)\n",
        "\n",
        "sfm_scores.sort_values(by='Feature Importance').plot(kind='barh', figsize=(8, 7)) # plot these as a bar plot\n",
        "plt.axvline(x=msfm, color='.5') # add the reference line y = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRxbLFBSZlZU"
      },
      "source": [
        "To view the features that are selected we can retrieve the output of the method `get_support()`. This returns a binary array which is True if the feature is selected and otherwise False. For example, this can then be used to display the selected features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz-uZwmHZlZU"
      },
      "outputs": [],
      "source": [
        "tfeatures.columns[selector.get_support()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYuFFwSYZlZU"
      },
      "source": [
        "Then we can create the reduced set of features using the `transform()` method. With this reduced dataset the accuracy and precision are slightly reduced, but the recall remains the same as the reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1sqj86VZlZU"
      },
      "outputs": [],
      "source": [
        "Xt = selector.transform(tfeatures)\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "accuracy, precision, recall = model_accuracy(rfc, Xt, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPqcvJ-WZlZU"
      },
      "source": [
        "## Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkEYw2PFZlZU"
      },
      "source": [
        "The function `RFE` (Recursive Feature Elimination) again uses the scores of an estimator, but recursively eliminates features until the required number of features are obtained. For example, on the first step the model is fitted and the feature which contributes the least as measured by the estimator scores is eliminated. The process is then repeated. At each step one fitting of the model occurs, but as the number of features is reduced, subsequent steps will take less time.\n",
        "\n",
        "`RFECV` implements this with cross-validation.\n",
        "\n",
        "The model is set-up in the same way as `SelectFromModel`. The two main parameters to specify are `n_features_to_select` and `step`, which respectively corresponds to the final number of features selected and the number of features eliminated on each step. Due to the number of iterations, this takes significantly longer to run compared to `SelectFromModel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "4HnVX90AZlZV",
        "outputId": "38f88b5b-38ad-40b8-dcf0-e72f235531c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f5f4b0640222>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_to_select\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'selector.fit(tfeatures, labels)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "estimator = RandomForestClassifier()\n",
        "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
        "%timeit selector.fit(tfeatures, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvNH_IYjZlZV"
      },
      "source": [
        "The relevant output of this model is the model is the ranking (`ranking_`), which details at which step that feature was eliminated. All features that are selected have a ranking of 1, the feature eliminated on the final step has a ranking of 2, and as the ranking increases that indicates the feature was eliminated on an earlier step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHj5cdqjZlZV"
      },
      "outputs": [],
      "source": [
        "df_scores = pd.DataFrame(selector.ranking_)\n",
        "df_columns = pd.DataFrame(tfeatures.columns)\n",
        "# concatenate dataframes\n",
        "rfe_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "rfe_scores.columns = ['Feature_Name','Feature Ranking']  # name output columns\n",
        "rfe_scores.set_index('Feature_Name', inplace=True)\n",
        "\n",
        "rfe_scores.sort_values(by='Feature Ranking', ascending=False).plot(kind='barh', figsize=(8, 7)) # plot these as a bar plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e22-7OkYZlZV"
      },
      "source": [
        "Again the selected features can be displayed using `get_support()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xhubbvRZlZV"
      },
      "outputs": [],
      "source": [
        "tfeatures.columns[selector.get_support()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC06oTWeZlZV"
      },
      "source": [
        "For this selection all the metrics for the model are reduced in comparison with the reference run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WustBOhjZlZV"
      },
      "outputs": [],
      "source": [
        "Xt = selector.transform(tfeatures)\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "accuracy, precision, recall = model_accuracy(rfc, Xt, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXcenebZZlZV"
      },
      "source": [
        "## Sequential Feature Selector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmFZGUPNZlZV"
      },
      "source": [
        "The final method considered here is `SequentialFeatureSelector`, which is new in Version 0.24 of sklearn. This has the advantages that it chooses features based on an accuracy measurement and can use cross-validation. The main disadvantage is that it is costly to run.\n",
        "\n",
        "`SequentialFeatureSelector` can be run in forward or backward mode. In forward mode at the first iteration the feature is chosen which maximizes the particular accuracy measurement. Then on the next iteration one feature is added which again maximizes the metric. This continues until the chosen number of features is reached. In backward mode the first iteration includes all features and features are eliminated at each step. By default 5-fold cross-validation is used.\n",
        "\n",
        "The setup is the same as previous models, except for the keywords `direction`, `scoring` and `cv`. `direction` controls whether forward or backward mode is used, with forward being the default. `scoring` controls the metric to be used. For example, we could use accuracy, precision or recall for classification, and r2_score for regression.\n",
        "\n",
        "Due to the large number of models that need to be fitted, the runtime is significantly longer than for `SelectFromModel` and `RFE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOS-8456ZlZV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "estimator = RandomForestClassifier()\n",
        "selector = SequentialFeatureSelector(estimator, n_features_to_select=5, scoring='recall')\n",
        "%timeit selector.fit(tfeatures, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mceOhk_nZlZV"
      },
      "source": [
        "Again, we can view the chose features using `get_support()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD_OAdBCZlZV"
      },
      "outputs": [],
      "source": [
        "tfeatures.columns[selector.get_support()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P8OdWF9ZlZV"
      },
      "source": [
        "Now although the model takes longer to run, the accuracy measurements are all at least as good as for the reference run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxUmioq0ZlZV"
      },
      "outputs": [],
      "source": [
        "Xt = selector.transform(tfeatures)\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "accuracy, precision, recall = model_accuracy(rfc, Xt, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMuVRVvvZlZV"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ZD6G1SZlZV"
      },
      "source": [
        "For the exercises we will use the [Boston Housing dataset](https://www.kaggle.com/c/boston-housing), which is part of the sklearn datasets. The function below fits an sklearn regression models, such as linear regresssion, SVR or Random Forest Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk6N4Q-PZlZV"
      },
      "outputs": [],
      "source": [
        "def reg_model_accuracy(model, features, labels, randomnumb=42):\n",
        "    '''Convenience function to fit an sklearn regression model and calculate the r2 score'''\n",
        "    import numpy as np\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import r2_score # import the score functions\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=randomnumb)\n",
        "    pipe = make_pipeline(StandardScaler(), model)\n",
        "    pipe.fit(X_train, y_train)  # apply scaling on training data\n",
        "    y_pred= pipe.predict(X_test) # calculate the predicted values of the model for the test features\n",
        "\n",
        "    rsq = np.round(r2_score(y_test, y_pred),3)\n",
        "\n",
        "    return rsq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdvQoXatZlZW"
      },
      "source": [
        "Load and display the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FSD-71RZZlZW",
        "outputId": "d9572a96-59b4-4850-988c-935415e5234e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  \\\n",
              "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
              "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
              "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
              "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
              "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
              "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
              "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
              "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
              "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
              "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
              "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
              "\n",
              "     PTRATIO       B  LSTAT  MEDV  \n",
              "0       15.3  396.90   4.98  24.0  \n",
              "1       17.8  396.90   9.14  21.6  \n",
              "2       17.8  392.83   4.03  34.7  \n",
              "3       18.7  394.63   2.94  33.4  \n",
              "4       18.7  396.90   5.33  36.2  \n",
              "..       ...     ...    ...   ...  \n",
              "501     21.0  391.99   9.67  22.4  \n",
              "502     21.0  396.90   9.08  20.6  \n",
              "503     21.0  396.90   5.64  23.9  \n",
              "504     21.0  393.45   6.48  22.0  \n",
              "505     21.0  396.90   7.88  11.9  \n",
              "\n",
              "[506 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68d0031c-07b3-45ed-a690-26e10c03515a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "      <td>22.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1</td>\n",
              "      <td>273</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "      <td>11.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68d0031c-07b3-45ed-a690-26e10c03515a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68d0031c-07b3-45ed-a690-26e10c03515a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68d0031c-07b3-45ed-a690-26e10c03515a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1cb830fc-72be-41e8-a195-bff7ce2edc1f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1cb830fc-72be-41e8-a195-bff7ce2edc1f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1cb830fc-72be-41e8-a195-bff7ce2edc1f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Boston = pd.read_csv('housing.csv')\n",
        "BostonFeatures = Boston.drop(columns=['MEDV'])\n",
        "BostonLabels = Boston['MEDV']\n",
        "Boston"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg3xPRc8ZlZW"
      },
      "source": [
        "### Exercise 1 (2 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJrvh9NPZlZW"
      },
      "source": [
        "Create a base regression model using the RandomForestRegressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGvdMUGAZlZW",
        "outputId": "35119297-2322-42c4-bdf0-7ac06d6048f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.865\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "randomforestreg = RandomForestRegressor()\n",
        "rsq = reg_model_accuracy(randomforestreg, BostonFeatures, BostonLabels)\n",
        "print(rsq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko9En1ofZlZW"
      },
      "source": [
        "### Exercise 2 (4 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UleLk4JvZlZW"
      },
      "source": [
        "Use SelectKBest using f_regression to select the best 5 features. Print out these features and determine the accuracy of the RandomForestRegressor with this reduced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_0BVGvZZlZW",
        "outputId": "2ab2249a-b4c3-471b-cc6d-02b0f5cc094f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['INDUS', 'RM', 'TAX', 'PTRATIO', 'LSTAT'], dtype='object')\n",
            "0.774\n"
          ]
        }
      ],
      "source": [
        "selector = SelectKBest(f_regression, k=5)\n",
        "features_new = selector.fit_transform(BostonFeatures, BostonLabels)\n",
        "print(BostonFeatures.columns[selector.get_support()])\n",
        "\n",
        "linreg = RandomForestRegressor()\n",
        "rsq = reg_model_accuracy(linreg, features_new, BostonLabels)\n",
        "print(rsq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftnAY7fsZlZW"
      },
      "source": [
        "### Exercise 3 (4 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqB7tAD8ZlZW"
      },
      "source": [
        "Repeat Exercise 2, but use RFE (Recursive Feature Elimination)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4j0zZMkZlZW",
        "outputId": "a02fe1da-a80a-46cc-e26f-0afa0db135f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['CRIM', 'NOX', 'RM', 'DIS', 'LSTAT'], dtype='object')\n",
            "0.839\n"
          ]
        }
      ],
      "source": [
        "estimator = RandomForestRegressor()\n",
        "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
        "features_new = selector.fit_transform(BostonFeatures, BostonLabels)\n",
        "print(BostonFeatures.columns[selector.get_support()])\n",
        "\n",
        "linreg = RandomForestRegressor()\n",
        "rsq = reg_model_accuracy(linreg, features_new, BostonLabels)\n",
        "print(rsq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRzekZGZlZW"
      },
      "source": [
        "### Generative AI Acknowledgement\n",
        "\n",
        "If you used generative AI to assist you with these excercises, document your use below following the guidelines here: https://www.monash.edu/learnhq/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-mRemVEZlZW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}